<div id="section-publications" class="section">
  <heading>Projects</heading>
  <table class="content-table">
    <tbody>

      <tr onmouseout="malle_stop10()" onmouseover="malle_start10()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image10'>
              <img src='./images/roboviolinist/teaser.png' width="190">
            </div>
            <img src='./images/roboviolinist/architecture_diagram.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start10() {
              document.getElementById('malle_image10').style.opacity = "1";
            }
            function malle_stop10() {
              document.getElementById('malle_image10').style.opacity = "0";
            }
            malle_stop10()
          </script>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://boshi-an.github.io/RobotViolinist/">
            <papertitle>RobotViolinist: Challenging Human Dexterity in Simulated Violin Playing with Dexterous Hands</papertitle>
          </a>
          <br>

          <span class="author-focus"><u>Boshi An</u></span>,
          <a href="https://srl.ethz.ch/the-group/people/chenyu-yang.html" class="author-focus">Chenyu Yang</a>
          <a href="https://robert.katzschmann.de" class="author-focus">Robert Katzschmann</a>
          
          <br>
          <a href="https://boshi-an.github.io/RobotViolinist/">Project Page</a>
          /
          <a href="https://github.com/boshi-an/RobotViolinist">Code (Coming Soon)</a>
          <br>
          <em>Ongoing</em>

          <p></p>

          <p>
            Violin playing is a challenging task even for human dexterity, requiring thousands hours of practice.
            Both hands must be controlled with sub-millimeter positional accuracy and millinewton contact accuracy.

            We designed a framework for simulating violin playing with dexterous hands,
            featuring accurate contact physics,
            realistic acoustics from motion and a dataset for accurate retargeted human violin playing motion.

            We also designed hierarchical long-horizon learning algorithms to solve the violin playing task.
          </p>
        </td>
      </tr>

      <tr onmouseout="malle_stop8()" onmouseover="malle_start8()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image8'>
              <img src='images/arnold/fig1_overview.png' width="190">
            </div>
            <img src='images/arnold/radar_plot.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start8() {
              document.getElementById('malle_image8').style.opacity = "1";
            }

            function malle_stop8() {
              document.getElementById('malle_image8').style.opacity = "0";
            }
            malle_stop8()
          </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
          <a href="https://www.arxiv.org/pdf/2508.18066">
            <papertitle><br>Arnold: A Generalist Muscle Transformer Policy</papertitle>
          </a>
          <br>

          <span class="author-focus"><u>Boshi An*</u></span>,
          <a href="https://scholar.google.com/citations?user=Cv5lSo0AAAAJ&hl=it" class="author-focus">Alberto Silvio Chiappa*</a>,
          <a href="https://scholar.google.com/citations?user=Ucq29dcAAAAJ&hl=el" class="author-focus">Merkourios Simos</a>,
          <a href="https://charlieleee.github.io" class="author-focus">Chengkun Li</a>,
          <a href="https://www.mathislab.org/" class="author-focus">Alexander Mathis</a>
          <br>

          <a href="https://www.arxiv.org/pdf/2508.18066">ArXiv</a>
          /
          <a href="">Project Page</a>
          /
          <a href="https://github.com/amathislab/arnold-the-generalist">Code</a>
          <br>
          <em>Under Review</em>

          <p></p>

          <p>
            Arnold is a single generalist muscle control policy trained on 14 different muscle control tasks,
            with 4 different embodiments. We used parallel on-policy behavior cloning, RL fine-tuning and self-distillation
            to match or even surpass expert performance on all tasks.
          </p>
        </td>
      </tr>

      <tr onmouseout="malle_stop7()" onmouseover="malle_start7()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image7'>
              <img src='images/roboverse/roboverse.jpg' width="190">
            </div>
            <img src='images/roboverse/roboverse2.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start7() {
              document.getElementById('malle_image7').style.opacity = "1";
            }

            function malle_stop7() {
              document.getElementById('malle_image7').style.opacity = "0";
            }
            malle_stop7()
          </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
          <a href="https://github.com/RoboVerseOrg/RoboVerse">
            <papertitle><br>RoboVerse: Towards a Unified Platform, Dataset and
Benchmark for Scalable and Generalizable Robot Learning</papertitle>
          </a>
          <br>

          <span class="author-focus"><u>Co-first author</u> among 38 authors</span>

          <br>
          <a href="https://roboverse.wiki">Project Page</a>
          /
          <a href="https://github.com/RoboVerseOrg/RoboVerse">Code</a>
          <br>
          <em>RSS 2025</em>

          <p></p>
          <p>
            RoboVerse is a unified framework for robotic learning.
            It provides an API interface to run simulations with multiple back-ends, equipped with large-scale datasets and benchmarks
            for training and evaluating robotic learning algorithms.
          </p>
        </td>
      </tr>
     
      <tr onmouseout="malle_stop4()" onmouseover="malle_start4()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image4'>
              <img src='images/rgb_manip/RGBManip.jpg' width="190">
            </div>
            <img src='images/rgb_manip/RGBManip2.jpg' width="190">
          </div>
          <script type="text/javascript">
            function malle_start4() {
              document.getElementById('malle_image4').style.opacity = "1";
            }

            function malle_stop4() {
              document.getElementById('malle_image4').style.opacity = "0";
            }
            malle_stop4()
          </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
          <a href="https://rgbmanip.github.io">
            <papertitle><br>RGBManip:
              Monocular Image-based Robotic Manipulation through Active Object Pose Estimation</papertitle>
          </a>
          <br>

          <span class="author-focus"><u>Boshi An*</u></span>,
          <a href="https://gengyiran.github.io" class="author-focus">Yiran Geng*</a>,
          <a href="https://ck-kai.github.io" class="author-focus">Kai Chen*</a>,
          <a href="https://clorislili.github.io/clorisLi/" class="author-focus">Xiaoqi Li</a>,
          <a href="https://www.cse.cuhk.edu.hk/~qdou/" class="author-focus">Qi Dou</a>,
          <a href="https://zsdonghao.github.io" class="author-focus">Hao Dong</a>

          <br>
          <a href="https://arxiv.org/abs/2310.03478">ArXiv</a>
          /
          <a href="https://rgbmanip.github.io">Project Page</a>
          /
          <a href="https://github.com/hyperplane-lab/RGBManip">Code</a>
          <br>
          <em>ICRA 2024</em> <b>Oral</b>

          <p></p>
          <p>
            We achieved state-of-the-art manipulation performance by combining reinforcement learning and
            multi-view pose estimation.
          </p>
        </td>
      </tr>

      <tr onmouseout="malle_stop3()" onmouseover="malle_start3()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image3'>
              <img src='images/emergence_language/Emergence.jpeg' width="190">
            </div>
            <img src='images/emergence_language/Emergence2.jpeg' width="190">
          </div>
          <script type="text/javascript">
            function malle_start3() {
              document.getElementById('malle_image3').style.opacity = "1";
            }

            function malle_stop3() {
              document.getElementById('malle_image3').style.opacity = "0";
            }
            malle_stop3()
          </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
          <a href="">
            <papertitle><br>Learning Multi-object Positional Relations via Emergent Communication</papertitle>
          </a>
          <br>

          <a href="https://takenpeanut.github.io" class="author-focus">Yicheng Feng*</a>,
          <span class="author-focus"><u>Boshi An*</u></span>,
          <a href="https://z0ngqing.github.io" class="author-focus">Zongqing Lu</a>

          <br>
          <a href="https://arxiv.org/abs/2302.08084">ArXiv</a>
          /
          <a href="">Project Page (Coming Soon)</a>
          /
          <a>Code (Coming Soon)</a>
          <br>
          <em>AAAI 2024</em> <b>Oral</b>

          <p></p>
          <p>
            We investigated whether a compositional language for describing 2-D positional relation can
            emerge from communication under environmental pressure.
          </p>
        </td>
      </tr>

      <tr onmouseout="malle_stop6()" onmouseover="malle_start6()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image6'>
              <img src='images/bp_net/BPNet.png' width="190">
            </div>
            <img src='images/bp_net/BPNet3.jpeg' width="190">
          </div>
          <script type="text/javascript">
              function malle_start6() {
                document.getElementById('malle_image6').style.opacity = "1";
            }

              function malle_stop6() {
                document.getElementById('malle_image6').style.opacity = "0";
            }
            malle_stop6()
          </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2403.11270.pdf">
            <papertitle><br>Bilateral Propagation Network for Depth Completion</papertitle>
          </a>
          <br>
          <span class="author-focus">Jie Tang</span>,
          <span class="author-focus">Fei-Peng Tian</span>,
          <span class="author-focus"><u>Boshi An</u></span>,
          <span class="author-focus">Jian Li</span>,
          <span class="author-focus">Ping Tan</span>

          <br>
          <a href="https://arxiv.org/pdf/2403.11270.pdf">ArXiv</a>
          /
          <a href="https://colab.research.google.com/drive/1i8mHgtCGrphOn84q_eEBR-srUwye5971?usp=drive_link">Project Demo</a>
          /
          <a href="https://github.com/kakaxi314/BP-Net">Code</a>
          <br>
          <em>CVPR 2024</em>

          <p></p>
          <p>
            We proposed an image-guided depth completion model which is SOTA on NYUv2 dataset and won
            the first place on KITTI benchmark at the time of submission.
          </p>
        </td>
      </tr>

      <tr onmouseout="malle_stop1()" onmouseover="malle_start1()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image1'>
              <img src='images/contact_rich/all.png' width="190">
            </div>
            <img src='images/contact_rich/myo.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start1() {
              document.getElementById('malle_image1').style.opacity = "1";
            }
            function malle_stop1() {
              document.getElementById('malle_image1').style.opacity = "0";
            }
            malle_stop1()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://sites.google.com/view/myochallenge">
            <papertitle>Learning contact-rich manipulation
              using a musculoskeletal hand</papertitle>
          </a>
          <br>

          <span class="author-focus">Vittorio Caggiano*</span>,
          <span class="author-focus">Guillaume Durandau*</span>,
          <span>...</span>,
          <a href="https://boshi-an.github.io" class="author-focus">Boshi An</a>
          <span>...</span>,
          <span class="author-focus">Vikash Kumar</span>
          
          <br>
          <a href="https://proceedings.mlr.press/v220/caggiano23a/caggiano23a.pdf">Paper</a>
          <br>
          <em>PMLR, 2023</em>
          <p></p>

          <p>This is the summary publication to the MyoChallenge 2022, a competition on die reorientation and baoding balls. </p>
        </td>
      </tr>

      <tr onmouseout="malle_stop2()" onmouseover="malle_start2()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image2'>
              <img src='images/e2e_rl/E2E_3.png' width="190">
            </div>
            <img src='images/e2e_rl/E2E.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start2() {
              document.getElementById('malle_image2').style.opacity = "1";
            }

            function malle_stop2() {
              document.getElementById('malle_image2').style.opacity = "0";
            }
            malle_stop2()
          </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2209.12941">
            <papertitle><br>End-to-End Affordance Learning for Robotic Manipulation</papertitle>
          </a>
          <br>

          <a href="https://gengyiran.github.io" class="author-focus">Yiran Geng*</a>,
          <span class="author-focus"><u>Boshi An*</u></span>,
          <a href="https://geng-haoran.github.io" class="author-focus">Haoran Geng</a>,
          <a href="https://cypypccpy.github.io/" class="author-focus">Yuanpei Chen</a>,
          <a href="https://www.yangyaodong.com/" class="author-focus">Yaodong Yang</a>,
          <a href="https://zsdonghao.github.io" class="author-focus">Hao Dong</a>

          <br>
          <a href="https://arxiv.org/abs/2209.12941">ArXiv</a>
          /
          <a href="https://sites.google.com/view/rlafford/">Project Page</a>
          /
          <a href="https://github.com/hyperplane-lab/RLAfford">Code</a>
          <br>
          <em>ICRA 2023</em>

          <p></p>
          <p>In this study, we take advantage of visual affordance by using the contact information generated
            during
            the RL training process to predict contact maps of interest.
          </p>
        </td>
      </tr>

      <tr onmouseout="malle_stop9()" onmouseover="malle_start9()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image9'>
              <img src='images/myo_challenge/myo1.png' width="190">
            </div>
            <img src='images/myo_challenge/myo.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start9() {
              document.getElementById('malle_image9').style.opacity = "1";
            }
            function malle_stop9() {
              document.getElementById('malle_image9').style.opacity = "0";
            }
            malle_stop9()
          </script>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://sites.google.com/view/myochallenge">
            <papertitle>MyoChallenge: Die reorientation</papertitle>
          </a>
          <br>

          <span class="author-focus"><u>Boshi An</u></span>,
          <a href="https://gengyiran.github.io" class="author-focus">Yiran Geng</a>,
          <a href="https://github.com/Ivan-Zhong" class="author-focus">Yifan Zhong</a>,
          <a href="https://jijiaming.com" class="author-focus">Jiaming Ji</a>,
          <a href="https://cypypccpy.github.io/" class="author-focus">Yuanpei Chen</a>
          
          <br>
          <a href="https://sites.google.com/view/myochallenge">Challenge Page</a>
          /
          <a href="https://github.com/PKU-MARL/MyoChallenge">Code</a>
          /
          <a href="pdf/DieRotation_NIPS22.pdf">Slides</a></li>
          /
          <a href="https://sites.google.com/view/myochallenge#h.t3275626vjox">Talk</a></li>
          <br>
          <b>First Place in NeurIPS 2022 Challenge Track (1st in 340 submissions from 40 teams)</b>
          <p></p>

          <p>Reconfiguring a die to match desired goal orientations. This task require delicate coordination of
            various
            muscles to manipulate the die without dropping it. </p>
        </td>
      </tr>

    </tbody>
  </table>

  <table class="content-table">
    <tbody>
      <heading>Preprints</heading>

      <tr onmouseout="malle_stop11()" onmouseover="malle_start11()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='malle_image11'>
              <img src='images/robot_assistant/frame_matrix.jpg' width="190">
            </div>
            <img src='images/robot_assistant/frame_matrix.jpg' width="190">
          </div>
          <script type="text/javascript">
            function malle_start11() {
              document.getElementById('malle_image11').style.opacity = "1";
            }

            function malle_stop11() {
              document.getElementById('malle_image11').style.opacity = "0";
            }
            malle_stop11()
          </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
          <a href="/images/robot_assistant/Assistant_Robot_Arxiv.pdf" target="_blank" rel="noopener">
            <papertitle><br>Robotic Assistant: Completing Collaborative Tasks
with Dexterous Vision-Language-Action Models</papertitle>
          </a>
          <br>

          <span class="author-focus">Boshi An, Chenyu Yang, Robert Katzschmann</span>

          <br>
          <a href="/images/robot_assistant/Assistant_Robot_Arxiv.pdf" target="_blank" rel="noopener">ArXiv</a>
          <br>
          <em>Preprint</em>

          <p></p>
          <p>
            We adapt a pre-trained Vision-Language-Action (VLA) model (Open-VLA) for dexterous humanâ€“robot collaboration with minimal language prompting.
          </p>
        </td>
      </tr>

      <tr onmouseout="malle_stop5()" onmouseover="malle_start5()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <div class="two" id='malle_image5'>
            <img src='images/image_manip/ImageManip2.jpg' width="190">
          </div>
          <img src='images/image_manip/ImageManip.jpg' width="190">
        </div>
        <script type="text/javascript">
            function malle_start5() {
              document.getElementById('malle_image5').style.opacity = "1";
          }

            function malle_stop5() {
              document.getElementById('malle_image5').style.opacity = "0";
          }
          malle_stop5()
        </script>
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle">
        <a href="">
          <papertitle><br>ImageManip
            Image-based Robotic Manipulation with Affordance-guided Next View Selection</papertitle>
        </a>
        <br>
        <span class="author-focus">Xiaoqi Li</span>,
        <span class="author-focus">Yanzi Wang</span>,
        <span class="author-focus">Yan Zhao</span>,
        <span class="author-focus">Yaroslav Ponomarenko</span>,
        <span class="author-focus">Qianxu Wang</span>,
        <span class="author-focus">Haoran Lu</span>,
        <span class="author-focus"><u>Boshi An</u></span>,
        <span class="author-focus">Jiaming Liu</span>,
        <span class="author-focus">Hao Dong</span>

        <br>
        <a href="https://arxiv.org/abs/2310.09069">ArXiv</a>
        <br>
        <em>Preprint</em>

        <p></p>
        <p>
          We propose a framework that utilizes only RGB inputs and enables 3D articulated object manipulation.
        </p>
      </td>
    </tr>

    </tbody>
  </table>
</div>
