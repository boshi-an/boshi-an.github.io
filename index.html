<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- <script type="text/javascript" src="js/js_func.js"></script> -->
  <title>Boshi An | 安博施</title>

  <meta name="author" content="Boshi An">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon/icon2.jpg">
  <script>
    var _hmt = _hmt || [];
    (function () {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?a4292ca8f2fe5fd7dc6dfd78cc894aab";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <p style="text-align:center">
            <name>Boshi An | 安博施</name>
          </p>
          <p style="text-align:center">
            <a href="mailto:boshi_an@alumni.pku.edu.cn">Email</a>
            &nbsp/&nbsp
            <a href="https://scholar.google.com.hk/citations?user=-1ITQkwAAAAJ&hl=en&oi=ao">Google Scholar</a>
            &nbsp/&nbsp
            <a href="https://github.com/boshi-an/">Github</a>
            &nbsp/&nbsp
            <a href="https://www.linkedin.com/in/%E5%8D%9A%E6%96%BD-%E5%AE%89-2b3275236/">Linkedin</a>
            &nbsp/&nbsp
            <a href="images/contact/WeChat.jpg">WeChat</a>
          </p>
          <heading>Introduction</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <td style="padding:2.5%;width:68%;vertical-align:top">
                <p>
                  I am currently pursuing a master's degree at <a href="https://www.epfl.ch/en/" target="_blank">EPFL</a> and
                  joining the <a href="https://srl.ethz.ch" target="_blank">Soft Robotics Lab</a> at <a href="https://ethz.ch/en.html" target="_blank">ETH</a>
                  as a master's student, supervised by <a href="https://srl.ethz.ch" target="_blank">Prof. Robert Katzschmann</a>.
                  I received my honours degree (summa cum laude) from <a
                    href="https://cfcs.pku.edu.cn/research/turing_program/introduction1/index.htm" target="_blank">Turing class</a>
                  at the <a href="https://eecs.pku.edu.cn/" target="_blank">School of Computer Science</a>,
                  <a href="https://english.pku.edu.cn/" target="_blank">Peking University</a>, where
                  I was mainly advised by <a href="https://zsdonghao.github.io" target="_blank">Prof. Hao Dong</a> on
                  robotics learning.
                  I also collaborated closely with <a href="https://z0ngqing.github.io" target="_blank">Prof. Zongqing Lu</a> on
                  the spatial cognitive abilities of machine learning models.
                </p>
                <p>
                  Outside of academics, I spend much of my free time at the piano, particularly in classical chamber music.
                  During my undergraduate years I formed a piano trio with friends.
                  Currently I am collaborating with local soprano <i>Liudmila Arno</i>, actively performing in Lausanne, Switzerland.
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a><img style="width:85%;max-width:85%" alt="profile photo" src="images/selfie/photo2.png"
                    class="hoverZoomLink">
                </a>
              </td>

            </tbody>
          </table>

          <heading>Research</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <p>
                    I'm interested in reinforcement learning, robotics and computational neurology.
                  </p>
  
                  <p>
                    To me, the essence of robotics lies in agility and dexterity.
                  </p>
                  <p>
                    Across evolution, animals have refined motor control in a billion-year interplay
                    of body and nervous system — through a vast distributed training process governed by survival and death,
                    extending three orders of magnitude beyond humanity’s brief venture into language, logic, and mathematics,
                    the very emblems of our civilization.
  
                    The architecture of the brain itself inscribes this history: to motion are entrusted far more neurons than ever to words.
                  </p>
                  <p>
                    We take pride in having taught machines to mirror our language,
                    yet in doing so we are reminded of our limits:
                    for what we prize as the summit of intellect, evolution regards as a late ornament.
                  </p>
                  <p>
                    As humble pilgrims we walk, striving to grasp nature’s vast bequest in motor control,
                    to emulate its mastery, and perhaps one day, to venture beyond it.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <heading>Publications</heading>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="malle_stop8()" onmouseover="malle_start8()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image8'>
                      <br>
                      <img src='images/arnold/fig1_overview.png' width="190">
                    </div>
                    <br>
                    <img src='images/arnold/radar_plot.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start8() {
                      document.getElementById('malle_image8').style.opacity = "1";
                    }

                    function malle_stop8() {
                      document.getElementById('malle_image8').style.opacity = "0";
                    }
                    malle_stop8()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://google.com">
                    <papertitle><br>Arnold: A Generalist Muscle Transformer Policy</papertitle>
                  </a>
                  <br>

                  <span class="author-focus"><u>Boshi An*</u></span>,
                  <a href="https://scholar.google.com/citations?user=Cv5lSo0AAAAJ&hl=it" class="author-focus">Alberto Silvio Chiappa*</a>,
                  <a href="https://scholar.google.com/citations?user=Ucq29dcAAAAJ&hl=el" class="author-focus">Merkourios Simos</a>,
                  <a href="https://charlieleee.github.io" class="author-focus">Chengkun Li</a>,
                  <a href="https://www.mathislab.org/" class="author-focus">Alexander Mathis</a>

                  <br>
                  <a href="https://www.arxiv.org/pdf/2508.18066">ArXiv</a>
                  /
                  <a href="">Project Page</a>
                  /
                  <a href="https://github.com/amathislab/arnold-the-generalist">Code</a>
                  <br>
                  <em></em>

                  <p></p>
                  <p>
                    Arnold is a single generalist muscle control policy trained on 14 different muscle control tasks,
                    with 4 different embodiments. We used parallel on-policy behavior cloning, RL fine-tuning and self-distillation
                    to match or even surpass expert performance on all tasks.
                  </p>
                </td>
              </tr>
              
              <tr onmouseout="malle_stop7()" onmouseover="malle_start7()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image7'>
                      <br>
                      <img src='images/roboverse/roboverse.jpg' width="190">
                    </div>
                    <br>
                    <img src='images/roboverse/roboverse2.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start7() {
                      document.getElementById('malle_image7').style.opacity = "1";
                    }

                    function malle_stop7() {
                      document.getElementById('malle_image7').style.opacity = "0";
                    }
                    malle_stop7()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://github.com/RoboVerseOrg/RoboVerse">
                    <papertitle><br>RoboVerse: Towards a Unified Platform, Dataset and
Benchmark for Scalable and Generalizable Robot Learning</papertitle>
                  </a>
                  <br>

                  <span class="author-focus"><u>Co-first author</u> among 38 authors</span>

                  <br>
                  <!-- <a href="https://arxiv.org/abs/2310.03478">ArXiv</a> -->
                  <!-- / -->
                  <a href="https://roboverse.wiki">Project Page</a>
                  /
                  <a href="https://github.com/RoboVerseOrg/RoboVerse">Code</a>
                  <br>
                  <em>RSS 2025</em>

                  <p></p>
                  <p>
                    RoboVerse is a unified framework for robotic learning.
                    It provides an API interface to run simulations with multiple back-ends, equipped with large-scale datasets and benchmarks
                    for training and evaluating robotic learning algorithms.
                  </p>
                </td>
              </tr>
             
              <tr onmouseout="malle_stop4()" onmouseover="malle_start4()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image4'>
                      <br>
                      <img src='images/rgb_manip/RGBManip.jpg' width="190">
                    </div>
                    <br>
                    <img src='images/rgb_manip/RGBManip2.jpg' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start4() {
                      document.getElementById('malle_image4').style.opacity = "1";
                    }

                    function malle_stop4() {
                      document.getElementById('malle_image4').style.opacity = "0";
                    }
                    malle_stop4()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://rgbmanip.github.io">
                    <papertitle><br>RGBManip:
                      Monocular Image-based Robotic Manipulation through Active Object Pose Estimation</papertitle>
                  </a>
                  <br>

                  <span class="author-focus"><u>Boshi An*</u></span>,
                  <a href="https://gengyiran.github.io" class="author-focus">Yiran Geng*</a>,
                  <a href="https://ck-kai.github.io" class="author-focus">Kai Chen*</a>,
                  <a href="https://clorislili.github.io/clorisLi/" class="author-focus">Xiaoqi Li</a>,
                  <a href="https://www.cse.cuhk.edu.hk/~qdou/" class="author-focus">Qi Dou</a>,
                  <a href="https://zsdonghao.github.io" class="author-focus">Hao Dong</a>

                  <br>
                  <a href="https://arxiv.org/abs/2310.03478">ArXiv</a>
                  /
                  <a href="https://rgbmanip.github.io">Project Page</a>
                  /
                  <a href="https://github.com/hyperplane-lab/RGBManip">Code</a>
                  <br>
                  <em>ICRA 2024</em> <b>Oral</b>

                  <p></p>
                  <p>
                    We achieved state-of-the-art manipulation performance by combining reinforcement learning and
                    multi-view pose estimation.
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop3()" onmouseover="malle_start3()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image3'>
                      <br>
                      <img src='images/emergence_language/Emergence.jpeg' width="190">
                    </div>
                    <br>
                    <img src='images/emergence_language/Emergence2.jpeg' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start3() {
                      document.getElementById('malle_image3').style.opacity = "1";
                    }

                    function malle_stop3() {
                      document.getElementById('malle_image3').style.opacity = "0";
                    }
                    malle_stop3()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle><br>Learning Multi-object Positional Relations via Emergent Communication</papertitle>
                  </a>
                  <br>

                  <a href="https://takenpeanut.github.io" class="author-focus">Yicheng Feng*</a>,
                  <span class="author-focus"><u>Boshi An*</u></span>,
                  <a href="https://z0ngqing.github.io" class="author-focus">Zongqing Lu</a>

                  <br>
                  <a href="https://arxiv.org/abs/2302.08084">ArXiv</a>
                  /
                  <a href="">Project Page (Coming Soon)</a>
                  /
                  <a>Code (Coming Soon)</a>
                  <br>
                  <em>AAAI 2024</em> <b>Oral</b>

                  <p></p>
                  <p>
                    We investigated whether a compositional language for describing 2-D positional relation can
                    emerge from communication under environmental pressure.
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop6()" onmouseover="malle_start6()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image6'>
                      <br>
                      <img src='images/bp_net/BPNet.png' width="190">
                    </div>
                    <br>
                    <img src='images/bp_net/BPNet3.jpeg' width="190">
                  </div>
                  <script type="text/javascript">
                      function malle_start6() {
                        document.getElementById('malle_image6').style.opacity = "1";
                    }

                      function malle_stop6() {
                        document.getElementById('malle_image6').style.opacity = "0";
                    }
                    malle_stop6()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2403.11270.pdf">
                    <papertitle><br>Bilateral Propagation Network for Depth Completion</papertitle>
                  </a>
                  <br>
                  <span class="author-focus">Jie Tang</span>,
                  <span class="author-focus">Fei-Peng Tian</span>,
                  <span class="author-focus"><u>Boshi An</u></span>,
                  <span class="author-focus">Jian Li</span>,
                  <span class="author-focus">Ping Tan</span>

                  <br>
                  <a href="https://arxiv.org/pdf/2403.11270.pdf">ArXiv</a>
                  /
                  <a href="https://colab.research.google.com/drive/1i8mHgtCGrphOn84q_eEBR-srUwye5971?usp=drive_link">Project Demo</a>
                  /
                  <a href="https://github.com/kakaxi314/BP-Net">Code</a>
                  <br>
                  <em>CVPR 2024</em>

                  <p></p>
                  <p>
                    We proposed an image-guided depth completion model which is SOTA on NYUv2 dataset and won
                    the first place on KITTI benchmark at the time of submission.
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop1()" onmouseover="malle_start1()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image1'>
                      <br>
                      <br>
                      <img src='images/contact_rich/all.png' width="180" height="110">
                    </div>
                    <br>
                    <br>
                    <img src='images/contact_rich/myo.png' width="180" height="110">
                  </div>
                  <script type="text/javascript">
                    function malle_start1() {
                      document.getElementById('malle_image1').style.opacity = "1";
                    }
                    function malle_stop1() {
                      document.getElementById('malle_image1').style.opacity = "0";
                    }
                    malle_stop1()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/myochallenge">
                    <papertitle>Learning contact-rich manipulation
                      using a musculoskeletal hand</papertitle>
                  </a>
                  <br>

                  <span class="author-focus">Vittorio Caggiano*</span>,
                  <span class="author-focus">Guillaume Durandau*</span>,
                  <span>...</span>,
                  <a href="https://boshi-an.github.io" class="author-focus">Boshi An</a>
                  <span>...</span>,
                  <span class="author-focus">Vikash Kumar</span>
                  
                  <br>
                  <a href="https://proceedings.mlr.press/v220/caggiano23a/caggiano23a.pdf">Paper</a>
                  <br>
                  PMLR, 2023
                  <p></p>

                  <p>This is the summary publication to the MyoChallenge 2022, a competition on die reorientation and baoding balls. </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop2()" onmouseover="malle_start2()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image2'>
                      <br>
                      <img src='images/e2e_rl/E2E_3.png' width="180">
                    </div>
                    <br>
                    <img src='images/e2e_rl/E2E.png' width="140">
                  </div>
                  <script type="text/javascript">
                    function malle_start2() {
                      document.getElementById('malle_image2').style.opacity = "1";
                    }

                    function malle_stop2() {
                      document.getElementById('malle_image2').style.opacity = "0";
                    }
                    malle_stop2()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2209.12941">
                    <papertitle><br>End-to-End Affordance Learning for Robotic Manipulation</papertitle>
                  </a>
                  <br>

                  <a href="https://gengyiran.github.io" class="author-focus">Yiran Geng*</a>,
                  <span class="author-focus"><u>Boshi An*</u></span>,
                  <a href="https://geng-haoran.github.io" class="author-focus">Haoran Geng</a>,
                  <a href="https://cypypccpy.github.io/" class="author-focus">Yuanpei Chen</a>,
                  <a href="https://www.yangyaodong.com/" class="author-focus">Yaodong Yang</a>,
                  <a href="https://zsdonghao.github.io" class="author-focus">Hao Dong</a>

                  <br>
                  <a href="https://arxiv.org/abs/2209.12941">ArXiv</a>
                  /
                  <a href="https://sites.google.com/view/rlafford/">Project Page</a>
                  /
                  <a href="https://github.com/hyperplane-lab/RLAfford">Code</a>
                  <br>
                  <em>ICRA 2023</em>

                  <p></p>
                  <p>In this study, we take advantage of visual affordance by using the contact information generated
                    during
                    the RL training process to predict contact maps of interest.
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop9()" onmouseover="malle_start9()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image9'>
                      <br>
                      <br>
                      <img src='images/myo_challenge/myo1.png' width="180" height="120">
                    </div>
                    <br>
                    <br>
                    <img src='images/myo_challenge/myo.png' width="180" height="120">
                  </div>
                  <script type="text/javascript">
                    function malle_start9() {
                      document.getElementById('malle_image9').style.opacity = "1";
                    }
                    function malle_stop9() {
                      document.getElementById('malle_image9').style.opacity = "0";
                    }
                    malle_stop9()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/myochallenge">
                    <papertitle>MyoChallenge: Die reorientation</papertitle>
                  </a>
                  <br>

                  <span class="author-focus"><u>Boshi An</u></span>,
                  <a href="https://gengyiran.github.io" class="author-focus">Yiran Geng</a>,
                  <a href="https://github.com/Ivan-Zhong" class="author-focus">Yifan Zhong</a>,
                  <a href="https://jijiaming.com" class="author-focus">Jiaming Ji</a>,
                  <a href="https://cypypccpy.github.io/" class="author-focus">Yuanpei Chen</a>
                  
                  <br>
                  <a href="https://sites.google.com/view/myochallenge">Challenge Page</a>
                  /
                  <a href="https://github.com/PKU-MARL/MyoChallenge">Code</a>
                  /
                  <a href="pdf/DieRotation_NIPS22.pdf">Slides</a></li>
                  /
                  <a href="https://sites.google.com/view/myochallenge#h.t3275626vjox">Talk</a></li>
                  <br>
                  <b>First Place in NeurIPS 2022 Challenge Track (1st in 340 submissions from 40 teams)</b>
                  <p></p>

                  <p>Reconfiguring a die to match desired goal orientations. This task require delicate coordination of
                    various
                    muscles to manipulate the die without dropping it. </p>
                </td>
              </tr>

            </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <heading>Preprints</heading>

                <tr onmouseout="malle_stop5()" onmouseover="malle_start5()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image5'>
                      <br>
                      <img src='images/image_manip/ImageManip2.jpg' width="190">
                    </div>
                    <br>
                    <img src='images/image_manip/ImageManip.jpg' width="190">
                  </div>
                  <script type="text/javascript">
                      function malle_start5() {
                        document.getElementById('malle_image5').style.opacity = "1";
                    }

                      function malle_stop5() {
                        document.getElementById('malle_image5').style.opacity = "0";
                    }
                    malle_stop5()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle><br>ImageManip
                      Image-based Robotic Manipulation with Affordance-guided Next View Selection</papertitle>
                  </a>
                  <br>
                  <span class="author-focus">Xiaoqi Li</span>,
                  <span class="author-focus">Yanzi Wang</span>,
                  <span class="author-focus">Yan Zhao</span>,
                  <span class="author-focus">Yaroslav Ponomarenko</span>,
                  <span class="author-focus">Qianxu Wang</span>,
                  <span class="author-focus">Haoran Lu</span>,
                  <span class="author-focus"><u>Boshi An</u></span>,
                  <span class="author-focus">Jiaming Liu</span>,
                  <span class="author-focus">Hao Dong</span>

                  <br>
                  <a href="https://arxiv.org/abs/2310.09069">ArXiv</a>
                  <!-- / -->
                  <!-- <a>Project Page (Coming Soon)</a> -->
                  <!-- / -->
                  <!-- <a>Code (Coming Soon)</a> -->
                  <br>
                  <!-- <em>RAL</em>, Under Review -->

                  <p></p>
                  <p>
                    We propose a framework that utilizes only RGB inputs and enables 3D articulated object manipulation.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellpadding="10">
            <tbody>
              <heading>Experience</heading>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="images/icon/SRL.png" , width="80%"></td>
                <td width="90%" valign="center">

                  <strong>Soft Robotics Lab, ETH Zurich</strong>, Switzerland
                  <br> 2025.01 - Now
                  <br> <strong>Master Student in Robotics</strong>
                  <br> Research Advisor: Prof. <a href="https://srl.ethz.ch">Robert Katzschmann</a>
                </td>
              </tr>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="images/icon/EPFL.png" , width="80%"></td>
                <td width="90%" valign="center">

                  <strong>EPFL</strong>, Switzerland
                  <br> 2024.09 - Now
                  <br> <strong>Master Student in Robotics</strong>
                </td>
              </tr>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="images/icon/PKU.png" , width="80%"></td>
                <td width="90%" valign="center">

                  <strong>Turing Class, Peking University</strong>, China
                  <br> 2020.09 - 2024.07
                  <br> <strong>Undergraduate Student, Summa cum laude</strong>
                  <br> Research Advisor: Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a>
                </td>
              </tr>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/icon/EPFL.png" ,
                    width="80%"></td>
                <td width="90%" valign="center">

                  <strong>Adaptive Motor Control Lab, EPFL</strong>, Switzerland
                  <br> 2023.07 - 2023.08
                  <br> <strong>Research Intern</strong>
                  <br> Research Advisor: Prof. <a href="https://www.mathislab.org/">Alexander Mathis</a>
                </td>
              </tr>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/icon/Edinburgh.png" ,
                    width="80%"></td>
                <td width="90%" valign="center">

                  <strong>University of Edinburgh</strong>, United Kingdom
                  <br> 2023.09 - 2023.12
                  <br> <strong>Exchange Student</strong>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <br>
                <br>
                <heading>Selected Awards and Honors</heading>
                <br>
                <br>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <p>
                    <li>2024: Honours Degree in Science </li>
                  </p>
                  <p>
                    <li>2024: Outstanding Graduates Prize (Peking University) </li>
                  </p>
                  <p>
                    <li>2023: Distinguished Researcher Prize (Peking University) </li>
                  </p>
                  <p>
                    <li>2022: First Place in NeurIPS 2022 Challenge Track 🏅 </li>
                  </p>
                  <p>
                    <li>2022: John Hopcroft Scholarship </li>
                  </p>
                  <p>
                    <li>2020: Outstanding Freshman Scholarship </li>
                  </p>
                  <p>
                    <li>2019: Chinese Olympiad in Informatics (NOI) Gold Medalist and National Training Team member 🏅
                    </li>
                  </p>

                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <br>
                <heading>Misc</heading>
                <br>

                <p>
                  You can find my music recordings in
                  my <a href="https://space.bilibili.com/234091394/">Bilibili page</a> .
                </p>

              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    This template is a modification to <a href="https://jonbarron.info/">Jon Barron's website</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
